# cudaDBBP
This is a complete set of program I wrote number of years ago to implement a neural network in CUDA framework.

It is embedded with a neural network-based deep learning module independently. In traditional neural networks, Back propagation is used. Simply put, iterative algorithms are used to train the entire network, and initial settings are randomly set. Values are calculated in the output of the current network, and then change the parameters of the previous layers according to the difference between the current actual output and the marked output, until convergence (a gradient descent method as a whole). The most popular deep machine learning is a layer-wise training mechanism, the reason is that if the Back propagation mechanism is used for a deep network, the residual error propagation to the frontmost layer has become very small, and gradient diffusion occurs, which will affect the accuracy. At the same time, the first step of deep machine learning is not random initialization, but is obtained by learning the structure of the input data, so this initial value is closer to the global optimum, so that better results can be obtained.

Since neural networks require a large number of matrix operations, this system combines the parallel computing features of GPUs to increase computing speed on a large scale. The advantage of this system is its flexible deployment. It can be deployed in either a private cloud or a public cloud, which fully meets the different requirements of performance and cost. The module can also perform self-learning and continuous optimization in the process of data processing and analysis, thereby making data mining and prediction more accurate. Such a continuous virtuous circle can mine the maximum value of customer data. At the same time, the system has a wealth of interfaces, which makes it easy to connect with the customer's database. Due to the GPU's super multi-threaded parallel computing capabilities, this system uses NVIDIA's GPU to replace the CPU for numerical calculations, and has achieved good results. Compared with the CPU server, it achieves a thousand times speedup ratio. I wrote all the codes under CUDA, an open source framework for GPU programming opened by NVIDIA, mapped deep machine learning algorithms to the GPU, and built a set of multi-node GPU server clusters on this basis.  

The GPU management software (MIDDLEWARE) is SLURM. SLURM's powerful GPU thread management function can allocate the suspended training process to any idle GPU at any time. Linux shell scripts of compilation, training, database are also included.


-- Alec Li
